{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7rKdkg0NZ4q",
        "outputId": "aecab26f-0afa-47df-a4be-67abf5727de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: highway-env in c:\\users\\caste\\envs\\highway\\lib\\site-packages (1.6)\n",
            "Requirement already satisfied: scipy in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from highway-env) (1.8.1)\n",
            "Requirement already satisfied: gym>=0.25 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from highway-env) (0.25.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from highway-env) (1.5.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from highway-env) (1.23.4)\n",
            "Requirement already satisfied: pygame>=2.0.2 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from highway-env) (2.1.2)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from highway-env) (3.6.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from gym>=0.25->highway-env) (2.2.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from gym>=0.25->highway-env) (0.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->highway-env) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->highway-env) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->highway-env) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->highway-env) (9.2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->highway-env) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->highway-env) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->highway-env) (4.38.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->highway-env) (21.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from pandas->highway-env) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->highway-env) (1.16.0)\n",
            "Collecting git+https://github.com/carlosluis/stable-baselines3@75fd27e019aaf4dad612823b4544620df2c47844\n",
            "  Cloning https://github.com/carlosluis/stable-baselines3 (to revision 75fd27e019aaf4dad612823b4544620df2c47844) to c:\\users\\caste\\appdata\\local\\temp\\pip-req-build-zc7cf0yt\n",
            "  Resolved https://github.com/carlosluis/stable-baselines3 to commit 75fd27e019aaf4dad612823b4544620df2c47844\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: gym==0.25 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from stable-baselines3==1.6.1) (0.25.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from stable-baselines3==1.6.1) (1.23.4)\n",
            "Requirement already satisfied: torch>=1.11 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from stable-baselines3==1.6.1) (1.12.1)\n",
            "Requirement already satisfied: cloudpickle in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from stable-baselines3==1.6.1) (2.2.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from stable-baselines3==1.6.1) (1.5.1)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from stable-baselines3==1.6.1) (3.6.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from gym==0.25->stable-baselines3==1.6.1) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from torch>=1.11->stable-baselines3==1.6.1) (4.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->stable-baselines3==1.6.1) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->stable-baselines3==1.6.1) (9.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->stable-baselines3==1.6.1) (1.0.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->stable-baselines3==1.6.1) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->stable-baselines3==1.6.1) (4.38.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->stable-baselines3==1.6.1) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->stable-baselines3==1.6.1) (21.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from matplotlib->stable-baselines3==1.6.1) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from pandas->stable-baselines3==1.6.1) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\caste\\envs\\highway\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==1.6.1) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/carlosluis/stable-baselines3 'C:\\Users\\caste\\AppData\\Local\\Temp\\pip-req-build-zc7cf0yt'\n",
            "  Running command git rev-parse -q --verify 'sha^75fd27e019aaf4dad612823b4544620df2c47844'\n",
            "  Running command git fetch -q https://github.com/carlosluis/stable-baselines3 75fd27e019aaf4dad612823b4544620df2c47844\n",
            "  Running command git checkout -q 75fd27e019aaf4dad612823b4544620df2c47844\n"
          ]
        }
      ],
      "source": [
        "# Install environment and agent\n",
        "!pip install highway-env\n",
        "# TODO: we use the bleeding edge version because the current stable version does not support the latest gym>=0.21 versions. Revert back to stable at the next SB3 release.\n",
        "!pip install git+https://github.com/carlosluis/stable-baselines3@75fd27e019aaf4dad612823b4544620df2c47844"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gp_URtXOYof",
        "outputId": "ab801024-6f55-40c5-b51e-2df9c1902cc6"
      },
      "outputs": [],
      "source": [
        "# Environment\n",
        "import gym\n",
        "import highway_env\n",
        "\n",
        "# Agent\n",
        "from stable_baselines3 import PPO, SAC\n",
        "from stable_baselines3.common.evaluation import evaluate_policy # Test models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQZshbZSOc8r",
        "outputId": "ff9bb8d5-77c4-4d7f-8fd8-bac7d5276199"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"racetrack-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiramente, vamos treinar agentes dados parâmetros mais comuns e avaliar os resultados obtidos para cada um.\n",
        "Posteriormente, para os mesmos algoritmos testados, usaremos o `Optuna` para tunar hiperparâmetros e avaliá-los. Por fim, criaremos agentes de tais algoritmos usando hiperparâmetros tunados no <a href= \"https://github.com/DLR-RM/rl-baselines3-zoo\">RL Zoo</a> para problemas semelhantes. Os algoritmos usados serão o `PPO` e o `SAC`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Avaliando modelos com os parâmetros padrões"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agents with default parameters\n",
        "sac_model_default = SAC('MlpPolicy', env)\n",
        "ppo_model_default = PPO('MlpPolicy', env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any, Dict\n",
        "\n",
        "def learn_and_evaluate(model: Any, env: gym.Env, n_timesteps: int | float, n_episodes: int, default: bool, algorithms_results: dict) -> Dict[str, dict]:\n",
        "    \"\"\"Function destinated to train and evaluate agents\"\"\"\n",
        "    \n",
        "    if default:\n",
        "        string_aux = ' default'\n",
        "    else:\n",
        "        string_aux = ' tunned'\n",
        "    model_name = str(model.__class__).split('.')[-1][:3] + string_aux\n",
        "\n",
        "    model.learn(n_timesteps)\n",
        "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=n_episodes)\n",
        "    algorithms_results[model_name] = {}\n",
        "    algorithms_results[model_name]['mean_reward'] = mean_reward\n",
        "    algorithms_results[model_name]['std_reward'] = std_reward\n",
        "\n",
        "    return algorithms_results\n",
        "\n",
        "algorithms_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'SAC default': {'mean_reward': 449.29054162614045,\n",
              "  'std_reward': 481.6294687877488},\n",
              " 'PPO default': {'mean_reward': 525.7549042252824,\n",
              "  'std_reward': 438.0120456032163}}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "algorithms_results = learn_and_evaluate(sac_model_default, env, 2e4, 10, default=True, algorithms_results=algorithms_results)\n",
        "algorithms_results = learn_and_evaluate(ppo_model_default, env, 5e4, 10, default=True, algorithms_results=algorithms_results)\n",
        "\n",
        "algorithms_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving trained models\n",
        "sac_model_default.save(path='models/sacDefault')\n",
        "ppo_model_default.save(path='models/ppoDefault')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load of the agents saved above\n",
        "sac_default_save = SAC.load('models/sacDefault')\n",
        "ppo_default_save = PPO.load('models/ppoDefault')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_model(env: gym.Env, model: Any) -> None:\n",
        "    \"\"\"Function to run a model and render the enviroment through time\"\"\"\n",
        "    \n",
        "    obs, done = env.reset(), False\n",
        "\n",
        "    i = 0\n",
        "    while not(done) and i < 300:\n",
        "        optAction, _states = model.predict(obs)\n",
        "        obs, reward, done, info = env.step(optAction)\n",
        "        env.render()\n",
        "        i += 1\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, vamos usar a função acima para vermos como os agentes estão se comportando no ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PPO simulation\n",
        "run_model(env, ppo_default_save)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SAC simulation\n",
        "run_model(env, sac_default_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analisando o obtido, vemos que o `PPO` não se comportou tão bem, pois conseguia manter uma velociade alta, mas a direção do carro parecia meio aleatória. Já o `SAC` tinha uma velocidade menor, mas tinha uma direção mais controlada, mas, mesmo assim, não é capaz de desviar do outro carro quando ambos estão na mesma faixa. Com isso, percebemos que nenhum dos dois é satisfatório para o proposto.\n",
        "\n",
        "Agora, faremos processo similar para modelos com parâmetros tunados, começando pelo `SAC`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Avaliando modelos com otimização de hiperparâmetros\n",
        "### SAC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro, faremos um estudo rápido a fim de, apenas, descobrir os parâmetros mais relevantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optimizerSAC import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-05 18:30:42,420]\u001b[0m A new study created in RDB with name: sacTest\u001b[0m\n",
            "\u001b[32m[I 2022-11-05 18:53:44,051]\u001b[0m Trial 0 finished with value: 107.30800413042307 and parameters: {'gamma': 0.95, 'learning_rate': 0.01, 'batch_size': 512, 'buffer_size': 100000, 'train_freq': 256, 'tau': 0.05, 'net_arch': 'medium'}. Best is trial 0 with value: 107.30800413042307.\u001b[0m\n",
            "\u001b[32m[I 2022-11-05 19:12:43,037]\u001b[0m Trial 1 finished with value: 20.09597761631012 and parameters: {'gamma': 0.9, 'learning_rate': 0.001, 'batch_size': 512, 'buffer_size': 10000, 'train_freq': 32, 'tau': 0.01, 'net_arch': 'small'}. Best is trial 0 with value: 107.30800413042307.\u001b[0m\n",
            "\u001b[32m[I 2022-11-05 19:24:49,072]\u001b[0m Trial 2 finished with value: 257.2470165563747 and parameters: {'gamma': 0.98, 'learning_rate': 0.01, 'batch_size': 64, 'buffer_size': 10000, 'train_freq': 16, 'tau': 0.001, 'net_arch': 'small'}. Best is trial 2 with value: 257.2470165563747.\u001b[0m\n",
            "\u001b[32m[I 2022-11-05 19:44:42,721]\u001b[0m Trial 3 finished with value: 15.001020169258117 and parameters: {'gamma': 0.98, 'learning_rate': 0.0001, 'batch_size': 32, 'buffer_size': 100000, 'train_freq': 256, 'tau': 0.05, 'net_arch': 'big'}. Best is trial 2 with value: 257.2470165563747.\u001b[0m\n",
            "\u001b[32m[I 2022-11-05 20:05:27,362]\u001b[0m Trial 4 finished with value: 15.660919225215912 and parameters: {'gamma': 0.995, 'learning_rate': 0.0001, 'batch_size': 512, 'buffer_size': 10000, 'train_freq': 32, 'tau': 0.01, 'net_arch': 'small'}. Best is trial 2 with value: 257.2470165563747.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "sac_test_study = optuna.create_study(\n",
        "    sampler=optuna.samplers.TPESampler(), pruner=None, direction='maximize', storage=\"sqlite:///sacTest.db\",\n",
        "    study_name=\"sacTest\", load_if_exists=True\n",
        ")\n",
        "sac_test_study.optimize(lambda trial: objectiveSAC(trial, 2, 5e3, 5), n_trials=5, catch=(AssertionError, ValueError))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('learning_rate', 0.20952476458406977),\n",
              "             ('train_freq', 0.19988291643976006),\n",
              "             ('batch_size', 0.1932064535823394),\n",
              "             ('tau', 0.1785067660504574),\n",
              "             ('gamma', 0.14762868394240056),\n",
              "             ('net_arch', 0.05310901917563603),\n",
              "             ('buffer_size', 0.01814139622533673)])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from optuna.importance import get_param_importances\n",
        "get_param_importances(sac_test_study)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Com isso, vemos que o `learning_rate` foi o mais importante para a otimização, enquanto que `buffer_size` e `net_arch` foram os menos influentes, assim, para a otimização do modelo, vamos desconsiderar estes parâmetros e trabalhar com os seus valores padrões. Só vamos excluí-los somente porque suas importâncias foram extremamente baixas e  pois é necessário excluir alguns parâmetros devido ao elevado tempo de execução da otimização, porque uma quantidade pequena de *trials* não deve ser muito conclusiva. Agora vamos rodar com mais *timesteps* e uma maior quantidade de *trials*, a fim de encontrar os melhores hiperparâmetros dentre os listados, para, posteriormente, criarmos um agente com tais valores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-06 10:28:19,598]\u001b[0m A new study created in RDB with name: sacStudy\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# SAC Optimization study\n",
        "sac_study = optuna.create_study(\n",
        "    sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_startup_trials=5), direction='maximize',\n",
        "    study_name='sacStudy', storage=\"sqlite:///sacstudy.db\", load_if_exists=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizing SAC hyperparameters\n",
        "sac_study.optimize(\n",
        "    lambda trial: objectiveSAC(trial, n_evaluations=4, n_timesteps=12e3, n_eval_episodes=15), n_trials=8, catch=(AssertionError, ValueError)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Devido a um limite de memória para o estudo, conseguiu-se fazer 8 *trials*, apenas, e o resultado foi o abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value:  846.5468435429036\n",
            "Params: \n",
            "  batch_size: 64\n",
            "  gamma: 0.9\n",
            "  learning_rate: 0.01\n",
            "  tau: 0.01\n",
            "  train_freq: 128\n"
          ]
        }
      ],
      "source": [
        "# Information about SAC best case\n",
        "trial = sac_study.best_trial\n",
        "\n",
        "print(\"Value: \", trial.value)\n",
        "\n",
        "print(\"Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, com os hiperparâmetros tunados do `SAC`, vamos criar um agente com tais parâmetros e treiná-los com 20.000 *timesteps*, uma quantidade  maior que a usada durante a otimização e depois ver como ele está se comportando no ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "opt_sac = SAC(\n",
        "    policy='MlpPolicy', env=env, gamma=0.9, batch_size=64, learning_rate=0.01, train_freq=128,\n",
        "    gradient_steps=128, tau=0.01\n",
        ")\n",
        "opt_algorithms_results = {}\n",
        "learn_and_evaluate(\n",
        "    opt_sac, env=env, n_timesteps=3e4, n_episodes=20, default=False, algorithms_results=opt_algorithms_results\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'SAC tunned': {'mean_reward': 777.3178841821849,\n",
              "  'std_reward': 647.9604158437256}}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "opt_algorithms_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "opt_sac.save(\"models/sacTuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading SAC tuned model\n",
        "opt_sac_save = SAC.load(\"models/sacTuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_model(env=env, model=opt_sac_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vendo seu comportamento no ambiente, vemos que o agente continua incapaz de desviar do outro carro, quando ambos os carros estão na mesma faixa. Entretanto, percebe-se uma melhora com relação ao primeiro agente criado, principalmente, quanto à velocidade atingida pelo carro. Assim, mesmo com a melhora significativa no retorno esperado médio, o agente continua ineficaz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro, faremos um estudo rápido a fim de, apenas, descobrir os parâmetros mais relevantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from optimizerPPO import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-05 20:12:07,422]\u001b[0m A new study created in RDB with name: ppoTest\u001b[0m\n",
            "\u001b[32m[I 2022-11-05 20:28:41,643]\u001b[0m Trial 0 finished with value: 94.08841642215847 and parameters: {'batch_size': 32, 'n_steps': 32, 'gamma': 0.98, 'learning_rate': 1.6434419844976428e-05, 'ent_coef': 1.8870841559669228e-08, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.92, 'net_arch': 'medium'}. Best is trial 0 with value: 94.08841642215847.\u001b[0m\n",
            "\u001b[32m[I 2022-11-05 20:44:40,774]\u001b[0m Trial 1 finished with value: 258.88079236522316 and parameters: {'batch_size': 8, 'n_steps': 8, 'gamma': 0.98, 'learning_rate': 0.008405684599925403, 'ent_coef': 2.915164906645702e-05, 'clip_range': 0.4, 'n_epochs': 20, 'gae_lambda': 0.92, 'net_arch': 'small'}. Best is trial 1 with value: 258.88079236522316.\u001b[0m\n",
            "\u001b[32m[I 2022-11-05 20:55:25,382]\u001b[0m Trial 2 finished with value: 221.96929724290968 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.995, 'learning_rate': 0.044335119736724056, 'ent_coef': 2.885210424618312e-07, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.9, 'net_arch': 'medium'}. Best is trial 1 with value: 258.88079236522316.\u001b[0m\n",
            "\u001b[32m[I 2022-11-05 21:09:41,493]\u001b[0m Trial 3 finished with value: 80.38124850392342 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.9, 'learning_rate': 0.00467240641779976, 'ent_coef': 1.5763586892910937e-06, 'clip_range': 0.4, 'n_epochs': 1, 'gae_lambda': 0.95, 'net_arch': 'small'}. Best is trial 1 with value: 258.88079236522316.\u001b[0m\n",
            "\u001b[33m[W 2022-11-05 21:21:02,702]\u001b[0m Trial 4 failed because of the following error: ValueError('Expected parameter loc (Tensor of shape (16, 1)) of distribution Normal(loc: torch.Size([16, 1]), scale: torch.Size([16, 1])) to satisfy the constraint Real(), but found invalid values:\\ntensor([[nan],\\n        [nan],\\n        [nan],\\n        [nan],\\n        [nan],\\n        [nan],\\n        [nan],\\n        [nan],\\n        [nan],\\n        [nan],\\n        [nan],\\n        [nan],\\n        [nan],\\n        [nan],\\n        [nan],\\n        [nan]], grad_fn=<AddmmBackward0>)')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\caste\\Envs\\highway\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"C:\\Users\\caste\\AppData\\Local\\Temp\\ipykernel_16380\\1508454015.py\", line 5, in <lambda>\n",
            "    ppo_test_study.optimize(lambda trial: objectivePPO(trial, 2, 1e4, 5), n_trials=5, catch=(AssertionError, ValueError))\n",
            "  File \"c:\\Users\\caste\\OneDrive\\Documentos\\USP\\Turing\\RL\\Highway\\optimizerPPO.py\", line 78, in objectivePPO\n",
            "    model.learn(n_timesteps, callback=eval_callback)\n",
            "  File \"c:\\Users\\caste\\Envs\\highway\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\", line 314, in learn\n",
            "    return super().learn(\n",
            "  File \"c:\\Users\\caste\\Envs\\highway\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\", line 272, in learn\n",
            "    self.train()\n",
            "  File \"c:\\Users\\caste\\Envs\\highway\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\", line 212, in train\n",
            "    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
            "  File \"c:\\Users\\caste\\Envs\\highway\\lib\\site-packages\\stable_baselines3\\common\\policies.py\", line 644, in evaluate_actions\n",
            "    distribution = self._get_action_dist_from_latent(latent_pi)\n",
            "  File \"c:\\Users\\caste\\Envs\\highway\\lib\\site-packages\\stable_baselines3\\common\\policies.py\", line 606, in _get_action_dist_from_latent\n",
            "    return self.action_dist.proba_distribution(mean_actions, self.log_std)\n",
            "  File \"c:\\Users\\caste\\Envs\\highway\\lib\\site-packages\\stable_baselines3\\common\\distributions.py\", line 153, in proba_distribution\n",
            "    self.distribution = Normal(mean_actions, action_std)\n",
            "  File \"c:\\Users\\caste\\Envs\\highway\\lib\\site-packages\\torch\\distributions\\normal.py\", line 54, in __init__\n",
            "    super(Normal, self).__init__(batch_shape, validate_args=validate_args)\n",
            "  File \"c:\\Users\\caste\\Envs\\highway\\lib\\site-packages\\torch\\distributions\\distribution.py\", line 55, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: Expected parameter loc (Tensor of shape (16, 1)) of distribution Normal(loc: torch.Size([16, 1]), scale: torch.Size([16, 1])) to satisfy the constraint Real(), but found invalid values:\n",
            "tensor([[nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan],\n",
            "        [nan]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "ppo_test_study = optuna.create_study(\n",
        "    sampler=optuna.samplers.TPESampler(), pruner=None, direction='maximize', storage=\"sqlite:///ppoTest.db\",\n",
        "    study_name=\"ppoTest\", load_if_exists=True\n",
        ")\n",
        "ppo_test_study.optimize(lambda trial: objectivePPO(trial, 2, 1e4, 5), n_trials=5, catch=(AssertionError, ValueError))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('batch_size', 0.18791876109031888),\n",
              "             ('learning_rate', 0.16264030721053566),\n",
              "             ('n_epochs', 0.14157810998445752),\n",
              "             ('n_steps', 0.1369247397346501),\n",
              "             ('ent_coef', 0.12533860051416498),\n",
              "             ('gamma', 0.08847227753641605),\n",
              "             ('clip_range', 0.0810378741656116),\n",
              "             ('gae_lambda', 0.04799848776046512),\n",
              "             ('net_arch', 0.028090842003380103)])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_param_importances(ppo_test_study)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vendo o obtido, percebe-se que `batch_size` foi o mais relevante, entretanto, a maioria dos hiperparâmetros apresentaram importância similar. Aqueles menos relevantes foram `net_arch` e `gae_lambda`, assim, pelos motivos explicados na otimização do `SAC`, desconsideraremos eles para achar os melhores valores dos parâmetros. Agora, faremos outro estudo com mais *trials* e *timesteps*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-07 20:09:46,677]\u001b[0m Using an existing study with name 'ppoStudy' instead of creating a new one.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# PPO Optimization study\n",
        "ppo_study = optuna.create_study(\n",
        "    sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_startup_trials=5), direction='maximize',\n",
        "    study_name='ppoStudy', storage=\"sqlite:///ppostudy.db\", load_if_exists=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizing PPO hyperparameters\n",
        "ppo_study.optimize(lambda trial: objectivePPO(trial, 4, 15e3, 15), n_trials=30, catch=(AssertionError, ValueError))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value:  616.6054743170738\n",
            "Params: \n",
            "  batch_size: 128\n",
            "  clip_range: 0.1\n",
            "  ent_coef: 0.01\n",
            "  gamma: 0.95\n",
            "  learning_rate: 0.0001\n",
            "  n_epochs: 20\n",
            "  n_steps: 32\n"
          ]
        }
      ],
      "source": [
        "# Information about PPO best case\n",
        "trial = ppo_study.best_trial\n",
        "\n",
        "print(\"Value: \", trial.value)\n",
        "\n",
        "print(\"Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Com os hiperparâmetros tunados, vamos criar o agente do algoritmo `PPO`, treiná-lo com os parâmetros encontrados, e ver o resultado no ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "opt_ppo = PPO(\n",
        "    policy='MlpPolicy', env=env, seed=5, gamma=0.95, batch_size=128, learning_rate=0.0001, n_epochs=20,\n",
        "    n_steps=32, ent_coef=0.01, clip_range=0.1\n",
        ")\n",
        "opt_algorithms_results ={}\n",
        "learn_and_evaluate(\n",
        "    opt_ppo, env=env, n_timesteps=1e5, n_episodes=20, default=False, algorithms_results=opt_algorithms_results\n",
        ")\n",
        "opt_ppo.save(\"models/ppoTuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'PPO tunned': {'mean_reward': 678.5849429577589,\n",
              "  'std_reward': 678.0447825283173}}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "opt_algorithms_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading PPO tuned model\n",
        "opt_ppo_save = PPO.load(\"models/ppoTuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_model(env=env, model=opt_ppo_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Acerca do obtido, podemos tirar conclusões semelhantes às feitas para o `SAC`, ou seja, que o agente apresentou melhora no quesito velocidade, entretanto, ainda não consegue desviar do outro carro. Comparado ao agente com parâmetros padrões, vemos que a melhora no controle da direção é significativa. Portanto, o modelo criado não atende ao que se espera dele."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agentes com hiperparâmetros do RL Zoo\n",
        "### SAC\n",
        "Os hiperparâmetros tunados para o `SAC` de alguns ambientes do *gym*, os quais foram considerados para essa parte, encontram-se <a href=\"https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/sac.yml\">aqui</a>. Os hiperparâmetros que servirão de base para testarmos nesse ambiente, são aqueles designados para a resolução do ambiente `MountainCarContinuous`, devido a uma certa similaridade entre esse ambiente e o `Highway` e por não ter uma quantidade muito alta de *timesteps*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'SAC tunned': {'mean_reward': 740.5834671095207,\n",
              "  'std_reward': 526.2893797126395}}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model with MountainCarContinuous tuned hyperparameters\n",
        "sac_mountain_zoo = SAC(\n",
        "    policy='MlpPolicy', env=env, learning_rate=3e-4, buffer_size=50000, batch_size=512, \n",
        "    ent_coef=0.1, gamma=0.9999, tau=0.01, train_freq=32, gradient_steps=32, learning_starts=0, use_sde=True, \n",
        "    policy_kwargs=dict(log_std_init=3.67, net_arch=[64, 64])\n",
        ")\n",
        "zoo_algorithms_results = {}\n",
        "learn_and_evaluate(sac_mountain_zoo, env=env, n_timesteps=5e4, n_episodes=30, default=False, algorithms_results=zoo_algorithms_results)\n",
        "sac_mountain_zoo.save(\"models/sacMountainZoo\")\n",
        "zoo_algorithms_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "sac_mountain_zoo_save = SAC.load(\"models/sacMountainZoo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_model(env, sac_mountain_zoo_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esse foi o melhor modelo até agora, o carro consegue andar numa velocidade boa e é o primeiro agente que ao se aproximar do carro da frente tenta trocar de direção. O único problema é que nem sempre essa troca de direção é feita da maneira correta, as vezes ele acaba por dar uma volta e andar no sentido oposto, em outras ele foge muito dos limites da pista, e em outras ele faz a ultrapassagem corretamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PPO\n",
        "Os hiperparâmetros tunados para o `PPO` de alguns ambientes do *gym*, os quais foram considerados para essa parte, encontram-se <a href=\"https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml\">aqui</a>. Os hiperparâmetros que servirão de base para testarmos nesse ambiente, são aqueles designados para a resolução do ambiente `MountainCarContinuous`, pelos mesmos motivos já apresentados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'PPO tunned': {'mean_reward': 215.40163235133514,\n",
              "  'std_reward': 227.03301917839454}}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ppo_mountain_zoo = PPO(\n",
        "    policy='MlpPolicy', env=env, learning_rate=7.7e-5, n_steps=8, batch_size=256, n_epochs=10, ent_coef=0.00429, \n",
        "    gamma=0.9999, gae_lambda=0.9, max_grad_norm=5, vf_coef=0.19,use_sde=True, clip_range=0.1,\n",
        "    policy_kwargs=dict(log_std_init=-3.29, ortho_init=False)\n",
        ")\n",
        "zoo_algorithms_results = {}\n",
        "learn_and_evaluate(ppo_mountain_zoo, env=env, n_episodes=30, n_timesteps=2e4, default=False, algorithms_results=zoo_algorithms_results)\n",
        "ppo_mountain_zoo.save(\"models/ppoMountainZoo\")\n",
        "zoo_algorithms_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppo_mountain_zoo_save = PPO.load(\"models/ppoMountainZoo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_model(env=env, model=ppo_mountain_zoo_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver que o resultado obtido foi pior do que o agente os com parâmetros padrões.\n",
        "## Conclusão\n",
        "Depois de treinarmos e observamos agentes em 3 situações distintas, uma com hiperparâmetros padrões, outra tunando alguns parâmetros, e, por fim, treinando um agente com hiperparâmetros já tunados em um ambiente similar, podemos concluir o seguinte:\n",
        "<ul>\n",
        "    <li>Comparando todos os agentes treinados, o melhor foi o SAC com parâmetros tunados do MountainCarContinuous;  </li>\n",
        "    <li>Apesar de observarmos uma melhora nos modelos com hiperparâmetros tunados localmente, no caso do PPO foi o melhor resultado, fazer tal otimização é muito custosa, por isso tivemos que restringir a quantidade de parâmetros considerados e a quantidade de timesteps. Assim, percebe-se que obter um resultado ótimo, de fato, não é algo tão simples de ser realizado;</li>\n",
        "</ul>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.7 ('highway')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e981c6c17e634db75794aa184de3e99ea1ffed7468c089ca528b990f89ee130e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
